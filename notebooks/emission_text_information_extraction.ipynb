{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42f189b-c617-4961-adc9-59971c325c55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pdfservices-sdk==2.3.1\n",
    "!pip install openpyxl\n",
    "!pip install langchain==0.0.278\n",
    "!pip install pdfservices-sdk\n",
    "!pip install openai==0.27.8\n",
    "!pip install jsonpickle \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf3484f-6a31-4ba2-842b-e16f5eb8f5ac",
     "showTitle": false,
     "title": ""
    },
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from adobe.pdfservices.operation.auth.credentials import Credentials\n",
    "\n",
    "from adobe.pdfservices.operation.execution_context import ExecutionContext\n",
    "from adobe.pdfservices.operation.io.file_ref import FileRef\n",
    "from adobe.pdfservices.operation.pdfops.extract_pdf_operation import \\\n",
    "ExtractPDFOperation\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_element_type import (\n",
    "    ExtractElementType,\n",
    ")\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_pdf_options import (\n",
    "    ExtractPDFOptions,\n",
    ")\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_renditions_element_type import (\n",
    "    ExtractRenditionsElementType,\n",
    ")\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "from functions.image_processing import *\n",
    "from functions.adobe import *\n",
    "from functions.text_prompt import *\n",
    "from functions.text_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb7508d-c023-4edf-83aa-8141c419fe4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fewshot prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ecd55a6-f570-4d10-8ac9-3bb990a2806b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples_emissions,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix_emissions,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7fff5b-6ccd-4ebc-995a-968bb55f181b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65202ee7-8de5-40da-a5f9-ee6c7bd5a035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = \"GPT4\"\n",
    "openai.api_base = os.getenv(\"OPENAI_BASE_GPT4\")\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY_GPT4\")\n",
    "openai.api_version = \"2024-02-15-preview\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"GPT4\",\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_base=openai.api_base,\n",
    "    model_kwargs={\"engine\": \"GPT4\"},\n",
    "    temperature=0,\n",
    "    # seed = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb9f463-a193-4a66-8d83-aafa6137964f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Now run the QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2702483f-cb4a-4e66-9413-08269a8afb27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_isa_emissions = pd.DataFrame()\n",
    "responses = []\n",
    "\n",
    "output_base_zip_path = \"/tmp/sdk_result/\"\n",
    "\n",
    "# ADOBE Pipeline\n",
    "if not os.path.exists(output_base_zip_path):\n",
    "    os.mkdir(output_base_zip_path)\n",
    "\n",
    "output_base_extract_folder = (\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/pdf\"\n",
    ")\n",
    "all_files = os.listdir(\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/pdf\"\n",
    ")\n",
    "all_files = glob.glob('/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/ISA_reports/OCI_Annual_Report_2022_vf*')\n",
    "to_run_file = [os.path.join(output_base_extract_folder, file) for file in all_files]\n",
    "\n",
    "# Processing each PDF\n",
    "for pdf_path in to_run_file:\n",
    "    pdf_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "    output_zip_path = os.path.join(output_base_zip_path, f\"{pdf_name}.zip\")\n",
    "    output_zipextract_folder = os.path.join(output_base_extract_folder, pdf_name)\n",
    "\n",
    "    # Check if the extraction folder already exists\n",
    "    if not os.path.exists(output_zipextract_folder):\n",
    "        # Run adobe API only if the extraction folder doesn't exist\n",
    "        adobeLoader(pdf_path, output_zip_path)\n",
    "    \n",
    "    documents = extract_text_from_file_adobe(output_zip_path, output_zipextract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9fccb9-381b-4636-8257-ba11b1b0657c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_emissions = \"\"\"\n",
    "Given the context provided, summarize the company's GHG emissions data for the\n",
    "latest reporting year:  including Scope 1, Scope 2, (include scope 2 market\n",
    "or location-based or both if reported), and Scope 3. Always includes GHG emissions\n",
    "if provided in the context. CO2 emissions are a type of GHG emissions. \n",
    "If the context includes only CO2 emissions data, use CO2 emissions\n",
    "numbers as the GHG emissions values. Do not overlook things. Pay extra attention\n",
    "to the data provided in json format in the context where GHG/CO2 emissions are\n",
    "usually reported. Do not confuse the Scope 1+2 with Scope 1 or Scope 2 \n",
    "emissions solely. Carefully read the context and extract the data exactly as\n",
    "it is presented. If the data is not stated in the context, leave it\n",
    "as null. DO NOT fabricate any numbers. It is very important to accurately\n",
    "reporting the data as per the guidelines provided. If you cannot extract the\n",
    "specific Scope 1, Scope 2, and Scope 3 GHG/CO2 emissions because the context\n",
    "does not explicitly state these figures, provide response with null values for\n",
    "the emissions data. Provide the answer in a structured JSON format.\n",
    "\"\"\"\n",
    "\n",
    "emissions_keywords = [\n",
    "    \"scope 1\",\n",
    "    \"scope 2\",\n",
    "    \"scope 3\",\n",
    "    \"greenhouse gas emissions\",\n",
    "    \"ghg\",\n",
    "    \"direct emissions\",\n",
    "    \"direct emission\",\n",
    "    \"indirect emissions\",\n",
    "    \"indirect emission\",\n",
    "    \"Scope 1\",\n",
    "    \"Scope 2\",\n",
    "    \"Scope 3\",\n",
    "    \"emission\",\n",
    "    \"emissions\",\n",
    "]\n",
    "\n",
    "df_isa_emissions = pd.DataFrame()\n",
    "responses = []\n",
    "\n",
    "output_base_zip_path = \"/tmp/sdk_result/\"\n",
    "\n",
    "# ADOBE Pipeline\n",
    "if not os.path.exists(output_base_zip_path):\n",
    "    os.mkdir(output_base_zip_path)\n",
    "\n",
    "output_update_extract_folder = (\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/pdf\"\n",
    ")\n",
    "output_base_extract_folder = (\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/adobe_raw\"\n",
    ")\n",
    "# all_files = os.listdir(\n",
    "    # \"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/\"\n",
    "# )\n",
    "all_files = glob.glob('/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/ISA_reports/*')\n",
    "to_run_file = [os.path.join(output_base_extract_folder, file) for file in all_files]\n",
    "to_run_file = list(set([sub.replace(\"-part1\", \"\").replace(\"-part2\", \"\").replace(\"-part3\", \"\") for sub in to_run_file]))\n",
    "\n",
    "# Processing each PDF\n",
    "for pdf_path in to_run_file:\n",
    "    \n",
    "    pdf_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "    output_zip_path = os.path.join(output_base_zip_path, f\"{pdf_name}.zip\")\n",
    "    output_zipextract_folder = os.path.join(output_base_extract_folder, pdf_name)\n",
    "    output_update_zipextract_folder = os.path.join(output_update_extract_folder, pdf_name)\n",
    "\n",
    "    # Check if the extraction folder already exists\n",
    "    if os.path.exists(output_zipextract_folder) or os.path.exists(output_update_zipextract_folder):\n",
    "        # print(pdf_path)\n",
    "        # adobeLoader(pdf_path, output_zip_path)\n",
    "\n",
    "        if os.path.exists(output_update_zipextract_folder):\n",
    "            print(f'process {output_update_zipextract_folder}')\n",
    "            documents = extract_text_from_file_adobe(output_zip_path, output_update_zipextract_folder)\n",
    "        else:\n",
    "            print(f'process {output_zipextract_folder}')\n",
    "            documents = extract_text_from_file_adobe(output_zip_path, output_zipextract_folder)\n",
    "\n",
    "        # QA GenAI pipeline\n",
    "        output, result_df = qa_unstructured(\n",
    "            documents,\n",
    "            query=query_emissions,\n",
    "            prompt_template=prompt_template,\n",
    "            keywords=emissions_keywords,\n",
    "            input_type=\"text\",\n",
    "        )\n",
    "        result_df[\"filename\"] = pdf_name\n",
    "\n",
    "        # Append results\n",
    "        df_isa_emissions = pd.concat([df_isa_emissions, result_df], ignore_index=True)\n",
    "        responses.append(output)\n",
    "\n",
    "        # Save the final df\n",
    "        df_isa_emissions.to_csv(\n",
    "            \"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/isa_extracted_emissions_0726.csv\",\n",
    "            index=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f86927-99cb-403d-a914-d7aaee7678ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "df_isa_emissions#['raw_text']#[df_isa_emissions['model_explanation'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec47710-956d-445d-89f5-4f02d809b236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv(\"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/isa_extracted_emissions_0724.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4050c1-8fb6-4dfc-a12f-86973e195a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "import ast\n",
    "pd.set_option('max_colwidth', 100)\n",
    "df = pd.read_csv(\"/dbfs/FileStore/projects/WSsustainability/data/validation/tmp/isa_extracted_emissions_0724.csv\")\n",
    "# df.loc[(df_null['emissions_reduction_targets'].isna()) & (df['GHG_reduction_targets'].notna()), 'emissions_reduction_targets'] = df['GHG_reduction_targets']\n",
    "df.drop(['error', 'net_scope_1', 'net_scope_2'], axis=1, inplace=True)\n",
    "df = df[~df['filename'].str.contains('part')]\n",
    "df_sub = df[(df['model_explanation'].notna())]\n",
    "# df_full = pd.concat([df_null, df_explode], axis=0, ignore_index=True)\n",
    "# df_full = df_full.sort_values('company_name')\n",
    "df_sub['reporting_year'] = df_sub['reporting_year'].astype('Int64').astype('str')\n",
    "df_sub = df_sub.fillna('None')\n",
    "# df_full = df_full.replace('<NA>', 'None', regex=False)\n",
    "\n",
    "# for col in ['target_year', 'base_year']:\n",
    "#     df_full[col] = df_full[col].astype(str).str.replace('FY', '').replace('2017-2019', '2017')\n",
    "# df_full['reduction_percentage'] = pd.to_numeric(df_full['reduction_percentage'], errors='coerce').astype(str)\n",
    "\n",
    "# df_full.rename(columns={'scope':'emissions_scope', 'target_year':'emissions_target_year', 'base_year':'emissions_base_year', 'target_type':'emissions_target_type', 'reduction_percentage':'emissions_reduction_percentage'}, inplace=True)\n",
    "# df_full = df_full.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68efaa81-3a70-4805-83b5-68660be39368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_truth = pd.read_excel('/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/feedback/isa_climate_energy_genAI_output.xlsx')\n",
    "df_truth = df_truth[df_truth['emissions_pipeline']=='text_table_pipeline']\n",
    "for col in ['emissions_scope_1', 'emissions_scope_2', 'emissions_scope_2_market',\n",
    "       'emissions_scope_2_location', 'emissions_scope_3']:\n",
    "    df_truth[col] =  pd.to_numeric(df_truth[col], errors='coerce')\n",
    "\n",
    "df_correct = df_truth[(df_truth['Feedback GHG Scope1']=='Correct') | (df_truth['Feedback GHG Scope2']=='Correct') |(df_truth['Feedback GHG Scope3']=='Correct')][['emissions_company', 'filename', 'emissions_scope_1', 'emissions_scope_2', 'emissions_scope_2_market', 'emissions_scope_2_location', 'emissions_scope_3', 'Feedback GHG Scope1', 'Feedback GHG Scope2', 'Feedback GHG Scope3']]\n",
    "df_correct = df_correct.fillna('None')\n",
    "df_correct = df_correct.drop_duplicates()\n",
    "\n",
    "df_merge = pd.merge(df_sub, df_correct, on=['filename'], how='left')\n",
    "# df_merge = pd.merge(df_merge, df_truth[['filename', 'custom_url']], on='filename', how='left')\n",
    "# df_merge.loc[df_merge['emissions_company'].notna(), 'validate'] = 'Correct'\n",
    "df_merge.loc[(df_merge['Feedback GHG Scope1']=='Correct') & (df_merge['scope_1']==df_merge['emissions_scope_1']), 'scope1_validate'] = 'Correct'\n",
    "df_merge.loc[(df_merge['Feedback GHG Scope2']=='Correct') & (df_merge['scope_2']==df_merge['emissions_scope_2']) &\n",
    "             (df_merge['scope_2_market']==df_merge['emissions_scope_2_market']) & (df_merge['scope_2_location']==df_merge['emissions_scope_2_location']), 'scope2_validate'] = 'Correct'\n",
    "df_merge.loc[(df_merge['Feedback GHG Scope3']=='Correct') & (df_merge['scope_3']==df_merge['emissions_scope_3']), 'scope3_validate'] = 'Correct'\n",
    "\n",
    "df_merge = df_merge.drop_duplicates().sort_values('company_name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d124787-1a5c-41db-b53c-535d1a320d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e759665-9e4a-4d8d-858f-c52fa3dbe4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ### Check consistency by running many times\n",
    "# results_validation = []\n",
    "# run_ids = []\n",
    "# total_runs = 5  # Total number of runs\n",
    "# df_isa_emissions_benchmark = pd.DataFrame()\n",
    "# responses_benchmark = []\n",
    "# output_base_zip_path = \"/tmp/sdk_result/\"\n",
    "# if not os.path.exists(output_base_zip_path):\n",
    "#     os.mkdir(output_base_zip_path)\n",
    "\n",
    "# output_base_extract_folder = (\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/adobe_raw\"\n",
    "# )\n",
    "# all_files = os.listdir(\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/adobe_raw\"\n",
    "# )\n",
    "# to_run_file = [os.path.join(output_base_extract_folder, file) for file in all_files]\n",
    "\n",
    "# # Processing each PDF\n",
    "# for pdf_path in to_run_file[:1]:\n",
    "#     pdf_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "#     output_zip_path = os.path.join(output_base_zip_path, f\"{pdf_name}.zip\")\n",
    "#     output_zipextract_folder = os.path.join(output_base_extract_folder, pdf_name)\n",
    "\n",
    "#     # Check if the extraction folder already exists\n",
    "#     if not os.path.exists(output_zipextract_folder):\n",
    "#         # Run adobe API only if the extraction folder doesn't exist\n",
    "#         adobeLoader(pdf_path, output_zip_path)\n",
    "\n",
    "#     documents = extract_text_from_file_adobe(output_zip_path, output_zipextract_folder)\n",
    "\n",
    "#     output_base_zip_path = \"/tmp/sdk_result/\"\n",
    "#     # Process the extracted documents\n",
    "#     for run_id in range(1, total_runs + 1):\n",
    "#         seed = run_id\n",
    "#         output, result_df = qa_unstructured(\n",
    "#             documents,\n",
    "#             query=query_emissions,\n",
    "#             prompt_template=prompt_template,\n",
    "#             keywords=emissions_keywords,\n",
    "#             seed=seed,\n",
    "#             input_type=\"text\",\n",
    "#         )\n",
    "#         result_df[\"filename\"] = pdf_name\n",
    "#         result_df[\"run_id\"] = run_id\n",
    "#         # Append results\n",
    "#         df_isa_emissions_benchmark = pd.concat(\n",
    "#             [df_isa_emissions_benchmark, result_df], ignore_index=True\n",
    "#         )\n",
    "#         responses_benchmark.append(output)\n",
    "\n",
    "# df_isa_emissions_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48587d2c-971b-47bd-a1cd-5aaf8cdf2e9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# input_txt = responses[0][\"input_documents\"]\n",
    "# with open(\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/input/input_txt.txt\",\n",
    "#     \"w\",\n",
    "# ) as file:\n",
    "#     for item in input_txt:\n",
    "#         file.write(str(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc9e8dff-b04a-4252-85d1-e7349841124d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Save the final df\n",
    "# df_isa_emissions.to_csv(\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/result/isa_extracted_emissions_1005.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "# df = pd.read_csv(\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/result/isa_extracted_emissions_1005.csv\",\n",
    "# )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "582fe92c-b3ac-404d-9c3c-1d7d5fe16ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # save and load json responses\n",
    "# # serialize with jsonpickle\n",
    "# serialized_documents = jsonpickle.encode(responses)\n",
    "# with open(\n",
    "#     \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/result/output_emissions_text_1005.json\",\n",
    "#     \"w\",\n",
    "# ) as file:\n",
    "#     file.write(serialized_documents)\n",
    "\n",
    "# file_path = \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/result/output_emissions_text_1005.json\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# # Deserialize the content back into Python objects\n",
    "# data = jsonpickle.decode(data)\n",
    "# data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2543055874605020,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "emission_text_information_extraction",
   "widgets": {}
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "table-parsing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
