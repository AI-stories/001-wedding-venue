{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1447a5fa-21f7-4db5-8a95-07ec14de4db2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Answering Planet Climate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae370c4-c5ae-4761-adf1-a00c01030477",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d01e0f2-8acb-42d5-900d-80884f8ecbd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!uv pip install openpyxl\n",
    "!uv pip install jsonpickle \n",
    "!uv pip install --upgrade langchain \n",
    "!uv pip install langchain-openai\n",
    "!uv pip install pydantic==1.10.10\n",
    "!uv pip install pdfservices-sdk==2.3.1\n",
    "!uv pip install qdrant-client\n",
    "!uv pip install --upgrade langchain-core \n",
    "!uv pip install --upgrade openai \n",
    "!uv pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c011aabc-b959-436a-a9ef-01f8a91b95cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea59ee4b-862d-4900-9b8c-06210f973399",
     "showTitle": false,
     "title": ""
    },
    "height": 149
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate, SerpAPIWrapper\n",
    "# Langchain imports\n",
    "from langchain.agents import (AgentExecutor, AgentOutputParser,\n",
    "                              LLMSingleActionAgent, Tool)\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "# Embeddings and vectorstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# Conversational memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.schema import (AgentAction, AgentFinish, HumanMessage,\n",
    "                              SystemMessage)\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai.embeddings.azure import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "from functions.adobe import *\n",
    "from functions.text_processing import * \n",
    "from functions.text_prompt import * \n",
    "from functions.planet_prompt import *\n",
    "from functions.tools import *\n",
    "\n",
    "openai_api_key=os.getenv(\"OPENAI_KEY_GPT4\")\n",
    "openai_api_base=os.getenv(\"OPENAI_BASE_GPT4\")\n",
    "openai.api_key = openai_api_key\n",
    "openai.api_base = openai_api_base\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2024-02-15-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241a36b1-72dc-4bb4-8e52-a062a7a21a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "import qdrant_client\n",
    "from qdrant_client import grpc \n",
    "from qdrant_client.conversions.conversion import RestToGrpc\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9564b0a3-853f-4755-9790-0335debaba8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    openai_api_key=openai_api_key,\n",
    "    azure_endpoint=openai_api_base,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c28eea-2d47-4dff-b1f2-c01c7bc07b30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "token_splitter = TokenTextSplitter(chunk_size=3000, chunk_overlap=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea36574-0f6b-4094-ab77-c2c47cdf4c83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment = \"text-embedding-3-large\",\n",
    "        openai_api_version = \"2023-05-15\",\n",
    "        model = \"text-embedding-3-large\",\n",
    "        api_key = openai_api_key,\n",
    "        azure_endpoint= openai_api_base\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3703e171-8d60-422f-ac1e-8745929f38a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question_groups = {\n",
    "    \"renewable energy target\": [\n",
    "        \"The client has set targets for its renewable energy usage\",\n",
    "        \"The client has a goal of using 100% renewable energy, regardless of organization growth\",\n",
    "        \"The client has met or is on track to meet its renewable energy target\"\n",
    "    ],\n",
    "    \"climate commitments\": [\n",
    "        \"The client has (publicly) committed to the Paris Climate Agreement/Net Zero/Decarbonisation to become carbon net zero by 2050 or earlier\",\n",
    "        \"The client has developed a climate/decarbonisation strategy including a action plan for reducing carbon emissions from its operations\",\n",
    "        \"The climate action plan is externally verified by an independent third-party (e.g. SBTi certified professional, accountant\",\n",
    "        \"In the externally verified climate action plan, at least 70% of its portfolio/business activities is in scope\",\n",
    "        \"The emission reduction target is considered net-zero and is science-based (i.e. follows the Net Zero Standard of the SBTI or has been approved by the SBTI)\"\n",
    "    ],\n",
    "    \"climate risks\": [\n",
    "    \"Climate change physical and transition risks: The client has conducted a climate risk assessment (preferably in line with TCFD recommendations) of its own operations to identify potential transition and physical risks, as well as climate solutions and opportunities\",\n",
    "    \"Climate change physical and transition risks: The client has conducted a climate risk assessment (preferably in line with TCFD recommendations) of its supply chain to identify potential transition and physical risks, as well as climate solutions and opportunities\"\n",
    "    ],\n",
    "    \"GHG emissions\": [\n",
    "        \"The client monitors and discloses its Scope 1 (operational) GHG emissions\",\n",
    "        \"The client monitors and discloses its GHG emissions associated with Scope 2 (purchased electricity)\",\n",
    "        \"The client monitors and discloses its Scope 3 emissions (indirect, value chain) either in total or in one or more material sources/categories\", ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ca744d-0802-4729-a97b-fdd4fb3d8031",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use the question relevant text to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf0e3f3-5e1c-484a-b832-2378d1869b13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_company_and_year(full_context):\n",
    "    # Take the first few pages of the full context\n",
    "    initial_pages = \"\\n\".join([doc.page_content for doc in full_context[:3]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Given the following text from the first few pages of a sustainability report, identify and return the name of the company that produced this report and the reporting year in a json format. The output must be in JSON format, with the following structure and keys strictly adhered to: \n",
    "                {{\n",
    "                \"Company\": \"The name of the company that produces the report\",\n",
    "                \"year\": \"The reporting year\"\n",
    "                }}  \n",
    "    Your JSON structure must follow the following principles and this is non-negotiable:\n",
    "        * Only return valid JSON\n",
    "        * Never include backtick symbols such as: `\n",
    "        * The response will be parsed with json.loads(), therefore it must be valid JSON.\n",
    "\n",
    "    Text:\n",
    "    {initial_pages}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts company names from sustainability reports.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "    \n",
    "\n",
    "example_template = \"\"\"\n",
    "---------question--------- \n",
    "{question}\n",
    "---------context--------- \n",
    "{context}\n",
    "---------answer---------\n",
    " {answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\", \"answer\"], template=example_template\n",
    ")\n",
    "\n",
    "def create_prompt(topic, question, context):\n",
    "    if topic in topic_question_data and question in topic_question_data[topic]:\n",
    "        data = topic_question_data[topic][question]\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            examples=data[\"example\"],\n",
    "            example_prompt=example_prompt,\n",
    "            prefix=f\"\"\"\n",
    "            You are a sustainability reporting specialist with expertise in evaluating corporate environmental practices. Use the following pieces of context to determine whether the statement is correct. If the document does not provide enough context to determine and you are really not sure about the answer, just say that you are unsure.\n",
    "\n",
    "            Context: {{context}}\n",
    "            Topic: {topic}\n",
    "            Statement: {question}\n",
    "            Specific Instructions: {data['instruction']}\n",
    "\n",
    "            Here are an example of how to analyze:\n",
    "            \"\"\",\n",
    "            suffix=\"\"\"\n",
    "             Make sure you also read and carefully follow these general instructions:\n",
    "            - Focus solely on the PARENT COMPANIES, ignoring information from subsidiaries, regions, or countries.\n",
    "            - The answer may not be straightforward, requiring combining contexts, the use of reasoning skills and professional judgment to interpret the information\n",
    "            - The output must be in JSON format, with the following structure and keys strictly adhered to: \n",
    "                {{\n",
    "                \"OBSERVATION\": \"Detailed observations based on analyzing the context and a motivation for your responses, keep it concise but also concrete and detailed enough. Don't be vague. Do not just repeat the statement. Use five sentences maximum.\",\n",
    "                \"FINAL_ANSWER\": 'Yes' or 'No',\n",
    "                \"SOURCES\": \"EXACT, UNMODIFIED EXCERPTS FROM THE GIVEN CONTEXT that support your findings. These extracts must match the source document character-for-character, including spaces, line breaks, and punctuation. DO NOT ALTER THE TEXT IN ANY WAY.  DO NOT ADD ANYTHING EXTRA. DO NOT INVENT ANYTHING. DO NOT WRITE YOUR OWN OBSERVATION HERE. EVERY STATEMENT MUST BE A VERBATIM QUOTE FROM THE PROVIDED CONTEXT. RETURN ALL RELEVANT REFERENCES you use for producing the answer. If absolutely nothing related can be found, respond with 'No relevant context found.'\"\n",
    "\n",
    "                }}\n",
    "            - Your JSON structure must follow the following principles and this is non-negotiable:\n",
    "                * Only return valid JSON\n",
    "                * Never include backtick symbols such as: `\n",
    "                * The response will be parsed with json.loads(), therefore it must be valid JSON.\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\"]\n",
    "        )\n",
    "        return few_shot_prompt.format(context=context)\n",
    "\n",
    "\n",
    "def analyze_question(question, question_relevant_excerpts, topic, question_index):\n",
    "    context = {\n",
    "        'context': question_relevant_excerpts,\n",
    "        # 'history': previous_answers,\n",
    "    }\n",
    "    \n",
    "    prompt = create_prompt(topic, question, context)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def create_retriever(documents: list[Document], k, lambda_mult):\n",
    "    # Create an Embeddings Instance of Azure OpenAI\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment = \"text-embedding-3-large\",\n",
    "        openai_api_version = \"2023-05-15\",\n",
    "        model = \"text-embedding-3-large\",\n",
    "        api_key = openai_api_key,\n",
    "        azure_endpoint= openai_api_base\n",
    "    )\n",
    "\n",
    "    # Create the vector store using Qdrant\n",
    "    db = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"my_documents\",\n",
    "        location=\":memory:\",\n",
    "    )\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": k, \"lambda_mult\": lambda_mult}\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "def retrieve_question_excerpts(question, full_context, k, lambda_mult):\n",
    "    \"\"\"\n",
    "    Retrieve specific excerpts relevant to the question from the topic context,\n",
    "    considering previous answers.\n",
    "    \"\"\"\n",
    "    # Create the retriever using the full context\n",
    "    retriever = create_retriever(full_context, k, lambda_mult)\n",
    "    \n",
    "    # Use the retriever to get the most relevant documents\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Concatenate the content of the retrieved documents\n",
    "    question_relevant_excerpts = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    return question_relevant_excerpts\n",
    "    \n",
    "def process_question_batch(question_groups, full_context,  k, lambda_mult):\n",
    "    results = []\n",
    "    company = extract_company_and_year(full_context)\n",
    "    for topic, topic_questions in question_groups.items():\n",
    "        # previous_answers = []\n",
    "        for index, question in enumerate(tqdm(topic_questions)):\n",
    "            questions_relevant_excerpts = retrieve_question_excerpts(question, full_context, k, lambda_mult)\n",
    "            analysis = analyze_question(question, questions_relevant_excerpts, topic, index)\n",
    "            \n",
    "            results.append({\n",
    "                \"company\": company,\n",
    "                \"topic\": topic,\n",
    "                \"question\": question,\n",
    "                \"analysis\": analysis,\n",
    "                \"relevant_excerpts\": questions_relevant_excerpts\n",
    "            })\n",
    "            \n",
    "            # Add this question and its answer to previous_answers for the next iteration\n",
    "            # previous_answers.append({\"question\": question, \"observation\": analysis})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ffd85e-e30a-4056-b8d1-7b33ca673ea1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "output_base_zip_path = \"/tmp/sdk_result/\"\n",
    "output_base_extract_folder = (\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/adobe_raw\"\n",
    ")\n",
    "all_files = os.listdir(\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/adobe_raw\"\n",
    ")\n",
    "to_run_file = [os.path.join(output_base_extract_folder, file) for file in all_files]\n",
    "\n",
    "# Processing each PDF\n",
    "for pdf_path in tqdm(to_run_file):\n",
    "    pdf_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "    output_zip_path = os.path.join(output_base_zip_path, f\"{pdf_name}.zip\")\n",
    "    output_zipextract_folder = os.path.join(output_base_extract_folder, pdf_name)\n",
    "    \n",
    "    try:\n",
    "        df = extract_text_from_file_adobe(output_zip_path, output_zipextract_folder)\n",
    "        texts = token_splitter.split_documents([\n",
    "            Document(page_content=row[1][\"text\"], metadata={\"source\": pdf_name, \"page\": row[1][\"page_number\"]})\n",
    "            for row in df.iterrows()\n",
    "        ])\n",
    "        results = process_question_batch(question_groups, texts, 7, 0.7)\n",
    "        df_results_company = pd.DataFrame(results)\n",
    "        df_results_company['pdf_name'] = pdf_name\n",
    "        df_results = pd.concat([df_results, df_results_company], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a130e55b-87b2-41d5-ae5c-c54b6e014d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"/dbfs/FileStore/projects/WSsustainability/data/validation/Planet/output/Planet_rag_2507.csv\",\n",
    "#     index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32fa9df-6c23-4a0a-b948-afd9b3408485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_parse_json(json_str):\n",
    "    if not isinstance(json_str, str):\n",
    "        print(f\"Input is not a string: {type(json_str)}\")\n",
    "        return None\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    json_str = json_str.strip()\n",
    "\n",
    "    # Check if the JSON string is wrapped with ```json or ```\n",
    "    if json_str.startswith('```json'):\n",
    "        json_str = json_str.replace('```json\\n', '').replace('\\n```', '')\n",
    "    elif json_str.startswith('```'):\n",
    "        json_str = json_str.replace('```\\n', '').replace('\\n```', '')\n",
    "\n",
    "    # Escape single quotes\n",
    "    json_str = json_str.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    # Remove newline characters and extra spaces\n",
    "    json_str = re.sub(r'\\s+', ' ', json_str)\n",
    "\n",
    "    # Ensure proper JSON format: remove trailing commas and fix curly braces\n",
    "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "    json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "\n",
    "    # Try to parse the JSON string\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return the original string wrapped in a dictionary\n",
    "        return {\"original_text\": json_str}\n",
    "\n",
    "def parse_json_columns(df, json_columns):\n",
    "    for col in json_columns:\n",
    "        print(f\"Processing column: {col}\")\n",
    "        # Clean and parse the JSON data\n",
    "        parsed_data = df[col].apply(lambda x: clean_and_parse_json(x))\n",
    "        \n",
    "        # Normalize the parsed JSON data\n",
    "        try:\n",
    "            normalized_data = pd.json_normalize(parsed_data)\n",
    "            # Concatenate the normalized data back to the original DataFrame\n",
    "            df = pd.concat([df.drop(columns=[col]), normalized_data], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing data for column {col}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Specify the columns containing JSON data\n",
    "json_columns = ['company', 'analysis']\n",
    "\n",
    "# Parse the JSON columns\n",
    "df_parsed = parse_json_columns(df_results, json_columns)\n",
    "df_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8dcf26-2dee-4588-84fd-9acd22337acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parsed.to_csv(\"/dbfs/FileStore/projects/WSsustainability/data/validation/Planet/output/Planet_rag_3107.csv\",\n",
    "    index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb4e87d-c25e-44b1-9562-756a8bcae97d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40acbc7-6147-4796-bd48-d6f2b4a98c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_url = pd.read_excel('/dbfs/FileStore/projects/WSsustainability/data/validation/ISA/search/isa_report_url_validation.xlsx', sheet_name='in')\n",
    "selected_rows = ['Yes','yes/?', '_x000D_Yes', '_x001D_Yes' ]\n",
    "df_url = df_url[df_url['Yes/No/?'].isin(selected_rows)]\n",
    "\n",
    "df_url['filename'] = df_url['encoded_url'].str.split('/').str[-1].str.replace('.pdf', '').str.replace(' ', '_').str.replace('-', '_')\n",
    "df_url = df_url[['encoded_url', 'filename']]\n",
    "df_url = df_url.drop_duplicates(subset = 'filename', keep = 'first')\n",
    "df_planet = pd.read_csv(\n",
    "    \"/dbfs/FileStore/projects/WSsustainability/data/validation/Planet/output/Planet_rag_3107.csv\"\n",
    ")\n",
    "df_planet['filename']=df_planet['pdf_name'].str.replace(' ', '_').str.replace('-', '_')\n",
    "df_output = pd.merge(df_planet, df_url, left_on='filename', right_on='filename', how='left')\n",
    "df_output = df_output.drop_duplicates()\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cfd8a32-7491-4039-862f-ba86702662ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_output.to_csv(\"/dbfs/FileStore/projects/WSsustainability/data/validation/Planet/output/Planet_rag_3107.csv\",\n",
    "    index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Planet_information_extraction_RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
